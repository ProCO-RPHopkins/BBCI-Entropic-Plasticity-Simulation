{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Neural Connectivity and Entropic Modulation: A BBCI Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate a simplified bidirectional brain-computer interface (BBCI) that:\n",
    "- **Simulates neural activity** in two brain regions (to mimic hemispheric isolation seen in ASD).\n",
    "- **Applies an entropy function** to one region to model enhanced neural malleability.\n",
    "- **Uses a simple machine learning model** (a feedforward neural network) to decode \"intent\" from neural activity.\n",
    "- **Implements neural stimulation** that boosts underactive neurons and synchronizes activity between regions.\n",
    "- **Visualizes and exports data** for further analysis.\n",
    "\n",
    "Each section is explained in plain language so that anyone—even with a basic background—can follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessary libraries.\n",
    "using Random              # For generating random numbers (simulating noise).\n",
    "using Flux: crossentropy                # For building and training the neural network.\n",
    "using Plots               # For plotting and visualizing the neural activity.\n",
    "using DataFrames, CSV     # For handling and exporting data as CSV files.\n",
    "using Statistics          # For calculating the mean and standard deviation.\n",
    "using Lux                 # For displaying DataFrames in a nice format.\n",
    "using Optimisers          # For training the neural network.\n",
    "using Zygote              # For training the neural network\n",
    "# using Flux: onecold, onehotbatch, onehot, throttle, @epochs, train! - Alternatives\n",
    "# using Flux: Chain, Dense, softmax, relu, batch, ADAM, params, testmode!, trainmode! - Alternatives     \n",
    "\n",
    "# Set a seed for reproducibility so that the random numbers remain the same on every run.\n",
    "Random.seed!(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating Neural Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate neural activity as a time series for two brain regions (Region 1 and Region 2).  \n",
    "\n",
    "- **Assumptions**  \n",
    "  - 100 time steps (each step can be thought of as a moment in time).  \n",
    "  - 10 neurons per region.  \n",
    "- **What we do**  \n",
    "  - Start with all neurons at zero activity.\n",
    "  - At each time step, we add a small random perturbation (noise) to each neuron's activity.\n",
    "  - For Region 2, we also add a small contribution from Region 1 to model weak connectivity (this represents hemispheric isolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation parameters.\n",
    "n_time = 100         # Total time steps.\n",
    "n_neurons = 10       # Number of neurons per region.\n",
    "\n",
    "# Initialize matrices (rows: time steps, columns: neurons) with zeros.\n",
    "region1 = zeros(n_time, n_neurons)\n",
    "region2 = zeros(n_time, n_neurons)\n",
    "\n",
    "# Simulate neural activity over time.\n",
    "for t in 2:n_time\n",
    "    # Update Region 1 activity: previous activity + small random noise.\n",
    "    region1[t, :] = region1[t-1, :] .+ 0.1 * randn(n_neurons)\n",
    "    \n",
    "    # Update Region 2 activity similarly.\n",
    "    region2[t, :] = region2[t-1, :] .+ 0.1 * randn(n_neurons)\n",
    "    \n",
    "    # Introduce weak connectivity: add a small fraction of Region 1's current activity.\n",
    "    region2[t, :] += 0.05 * region1[t, :]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhancing Neural Malleability Using an Entropy Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mimic the effects of enhanced neuroplasticity (as suggested by entropic brain theory), we create a function that:\n",
    "- **Adds extra noise (variability)** to the neural activity in Region 2.\n",
    "- **Entropy Level:** This is a parameter (default is 0.3) that controls how much extra noise is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×10 Matrix{Float64}:\n",
       "  0.0383716   0.145408   -0.140226   -0.0133453   …   0.109191    -0.0934207\n",
       " -0.0937368  -0.185792   -0.161061   -0.383018       -0.153697    -0.0385482\n",
       "  0.306138    0.0732529  -0.411326    0.188781       -0.123937     0.282411\n",
       "  0.055748    0.0274745  -0.275454    0.586147       -0.00756903  -0.446881\n",
       " -0.55564     0.312256   -0.065547   -0.261935        0.0348089   -0.265904\n",
       "  0.0531877   0.261876    0.477934    0.468423    …  -0.158896    -0.17455\n",
       " -0.778859    1.09266    -0.0752083   0.57958        -0.280189     0.203601\n",
       " -0.434859    0.799988    0.329738    0.155644       -0.661114     0.243939\n",
       " -0.484184    0.941222   -0.283017   -0.00229144     -0.398897     0.0861217\n",
       " -0.367567    0.543344    0.384814    0.256945       -0.225915     0.00157225\n",
       "  ⋮                                               ⋱               \n",
       "  1.99579    -4.00262    -3.22112    -2.64517        -0.161853     0.513809\n",
       "  2.18558    -4.05474    -2.70571    -2.08699        -0.365969     0.747713\n",
       "  1.60088    -3.92388    -2.82151    -3.30928        -0.807156     0.0777158\n",
       "  1.65857    -4.24282    -2.43554    -2.78178        -0.399776     0.106494\n",
       "  2.24532    -4.07598    -3.6115     -2.67377     …  -0.760609     0.212575\n",
       "  1.74195    -4.03839    -3.81517    -2.8131         -0.848341    -0.39701\n",
       "  1.91478    -3.48146    -3.70093    -2.85537        -0.635094    -0.0456007\n",
       "  2.28555    -4.3247     -3.14816    -2.62998        -0.519473    -0.220025\n",
       "  2.34276    -3.78046    -4.26444    -2.37023        -0.751426     0.262218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function that applies an \"entropy\" transformation.\n",
    "function apply_entropy(activity, entropy_level=0.3)\n",
    "    # Add extra random noise to simulate increased variability.\n",
    "    noisy_activity = activity .+ entropy_level * randn(size(activity)...)\n",
    "    return noisy_activity\n",
    "end\n",
    "\n",
    "# Apply the entropy function to Region 2.\n",
    "region2_entropy = apply_entropy(region2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intent Recognition with a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a simple feedforward neural network to “recognize intent” based on the neural activity from both regions.\n",
    "  \n",
    "**How to prepare the data**  \n",
    "- We combine the activity from both regions by taking the average (element-wise) so that each time step yields a single vector (of length 10).  \n",
    "- We then create labels based on whether the average firing rate is above a threshold (for demonstration purposes, we use a simple rule).\n",
    "\n",
    "**Neural Network Architecture**  \n",
    "- **Input layer:** 10 neurons (one per averaged firing rate).  \n",
    "- **Hidden layer:** 16 neurons with a ReLU activation function.  \n",
    "- **Output layer:** 2 neurons with softmax activation (to classify into 2 classes, e.g., \"intent 0\" or \"intent 1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×100 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " 1  1  1  1  1  1  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  1  1  1     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine activity from both regions by averaging each neuron's activity across regions.\n",
    "X_ml = (region1 .+ region2_entropy) ./ 2  # Resulting size: (n_time, n_neurons)\n",
    "\n",
    "# Create labels: if the mean activity of a time step is above 0.05, we label it as 1 (else 0).\n",
    "y = [mean(X_ml[t, :]) > 0.05 ? 1 : 0 for t in 1:n_time]\n",
    "\n",
    "# Flux models expect data as columns (features x samples), so transpose the data.\n",
    "X_ml = transpose(X_ml)  # Now size is (10, n_time)\n",
    "\n",
    "# Convert labels into one-hot encoding for classification.\n",
    "y_oh = Flux.onehotbatch(y, 0:1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Activation` not defined in `Lux`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Activation` not defined in `Lux`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] getproperty(x::Module, f::Symbol)\n",
      "   @ Base .\\Base.jl:42\n",
      " [2] top-level scope\n",
      "   @ c:\\Users\\ryan.hopkins\\Videos\\BCImodel\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# --- Model Definition ---\n",
    "model = Lux.Chain(\n",
    "    Lux.Dense(10, 16, relu),     # Input 10 → 16, ReLU activation\n",
    "    Lux.Dense(16, 2),            # Hidden 16 → 2 (logits)\n",
    "    Lux.Activation(softmax)      # Output probabilities with softmax\n",
    ")\n",
    "\n",
    "# Initialize model parameters and state\n",
    "rng = Random.default_rng()\n",
    "ps, st = Lux.setup(rng, model)\n",
    "\n",
    "# --- Loss Function ---\n",
    "function loss(ps, x, y)\n",
    "    # Apply model with parameters (ps), state (st), and input (x)\n",
    "    output, _ = Lux.apply(model, ps, st, x)\n",
    "    return crossentropy(output, y)\n",
    "end\n",
    "\n",
    "# --- Optimizer ---\n",
    "opt = Optimisers.Descent(0.01)  # Learning rate 0.01\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in 1:100\n",
    "    # Compute gradients\n",
    "    grads = Zygote.gradient(ps) do p\n",
    "        loss(p, X_ml, y_oh)\n",
    "    end\n",
    "    \n",
    "    # Update parameters\n",
    "    ps = Optimisers.update(opt, ps, grads)\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0\n",
    "        current_loss = loss(ps, X_ml, y_oh)\n",
    "        println(\"Epoch: $epoch, Loss: $current_loss\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Model training complete.\")\n",
    "\n",
    "#= using Random\n",
    "using Lux\n",
    "using Zygote\n",
    "using Optimisers\n",
    "using Flux: crossentropy  # We use Flux's crossentropy for the loss function\n",
    "\n",
    "# --- Data Assumptions ---\n",
    "# X_ml: a plain Array{Float32} of size (10, n_samples) representing the averaged firing rates per time step.\n",
    "# y_oh: a one-hot encoded matrix of size (2, n_samples) for two classes (\"intent 0\" and \"intent 1\").\n",
    "\n",
    "# Make sure X_ml is a plain Array{Float32} (in case it was a lazy transpose)\n",
    "X_ml = Float32.(Array(X_ml))\n",
    "\n",
    "# --- Model Definition ---\n",
    "# Neural network architecture:\n",
    "#   - Input layer: 10 neurons\n",
    "#   - Hidden layer: 16 neurons with ReLU activation\n",
    "#   - Output layer: 2 neurons with softmax activation\n",
    "model = Lux.Chain(\n",
    "    Lux.Dense(10, 16, relu),\n",
    "    Lux.Dense(16, 2),\n",
    "    softmax  # Using Flux's softmax; alternatively, you could use Lux.softmax\n",
    ")\n",
    "\n",
    "# Initialize model parameters (we get both parameters and cache, but we'll ignore the cache).\n",
    "rng = Random.default_rng()\n",
    "ps, _cache = Lux.setup(rng, model)\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Since our model is stateless, we use the 3-argument version of `Lux.apply`.\n",
    "function loss(ps, x, y)\n",
    "    # Call the model with the parameters and input.\n",
    "    # This uses the method: apply(model, ps, x)\n",
    "    output = Lux.apply(model, ps, x)\n",
    "    return crossentropy(output, y)\n",
    "end\n",
    "\n",
    "# --- Optimizer ---\n",
    "opt = Optimisers.Descent(0.01)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in 1:100\n",
    "    # Compute gradients with respect to the parameters.\n",
    "    grads = Zygote.gradient(ps) do p\n",
    "        loss(p, X_ml, y_oh)\n",
    "    end\n",
    "    # Update the parameters.\n",
    "    ps = Optimisers.update(opt, ps, grads)\n",
    "    \n",
    "    # Print loss every 10 epochs.\n",
    "    if epoch % 10 == 0\n",
    "        println(\"Epoch: $epoch, Loss: \", loss(ps, X_ml, y_oh))\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Model training complete.\") =#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Modulation via Stimulation and Synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the bidirectional control in a BBCI:\n",
    "- **Stimulation:** We boost the activity of neurons in Region 2 that have firing rates below a threshold (0.3). For these neurons, we add an extra 0.5 to their activity.\n",
    "- **Synchronization:** We then adjust Region 2’s activity so that it becomes more similar to Region 1 (by averaging both regions).\n",
    "\n",
    "This mimics how an external device might guide neural activity toward a healthier, more connected state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that stimulates underactive neurons.\n",
    "function stimulate_neural_activity(activity, threshold=0.3, stimulation=0.5)\n",
    "    # Make a copy to avoid changing the original data.\n",
    "    stimulated = copy(activity)\n",
    "    # Loop through every element and add stimulation if below the threshold.\n",
    "    for i in eachindex(activity)\n",
    "        if activity[i] < threshold\n",
    "            stimulated[i] += stimulation\n",
    "        end\n",
    "    end\n",
    "    return stimulated\n",
    "end\n",
    "\n",
    "# Apply stimulation to the entropy-modified Region 2.\n",
    "region2_stimulated = stimulate_neural_activity(region2_entropy)\n",
    "\n",
    "# Synchronize Region 2 to move its activity closer to Region 1 by averaging both.\n",
    "region2_sync = 0.5 .* region2_stimulated .+ 0.5 .* region1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Neural Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the neural activity over time for a representative neuron (Neuron 1) in:\n",
    "- **Region 1 (baseline)**\n",
    "- **Region 2 after applying entropy**\n",
    "- **Region 2 after stimulation and synchronization**\n",
    "\n",
    "This helps us visually understand how the neural activity changes with each intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time vector for plotting.\n",
    "time_steps = 1:n_time\n",
    "\n",
    "# Plot the activity for Neuron 1 from each dataset.\n",
    "plot(time_steps, region1[:, 1], label=\"Region 1 (Neuron 1)\", lw=2)\n",
    "plot!(time_steps, region2_entropy[:, 1], label=\"Region 2 with Entropy (Neuron 1)\", lw=2)\n",
    "plot!(time_steps, region2_sync[:, 1], label=\"Region 2 After Stimulation & Sync (Neuron 1)\", lw=2)\n",
    "xlabel!(\"Time Step\")\n",
    "ylabel!(\"Firing Rate\")\n",
    "title!(\"Neural Activity Over Time (Neuron 1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporting the Results to a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analysis or sharing, we export the following:\n",
    "- **Time step**\n",
    "- **Mean firing rate of Region 1**\n",
    "- **Mean firing rate of Region 2 after entropy**\n",
    "- **Mean firing rate of Region 2 after stimulation and synchronization**\n",
    "\n",
    "We use the DataFrames and CSV libraries to create a table and write it out to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Create a DataFrame to store the mean firing rates over time.\n",
    "df = DataFrame(\n",
    "    Time = time_steps,\n",
    "    Region1_Mean = [mean(region1[t, :]) for t in time_steps],\n",
    "    Region2_Entropy_Mean = [mean(region2_entropy[t, :]) for t in time_steps],\n",
    "    Region2_Sync_Mean = [mean(region2_sync[t, :]) for t in time_steps]\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a CSV file.\n",
    "CSV.write(\"neural_activity_results.csv\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we\n",
    "- **Simulated neural activity** in two brain regions with a built-in weak connectivity (to mimic hemispheric isolation).\n",
    "- **Applied an entropy function** to increase neural variability in Region 2, simulating enhanced neuroplasticity.\n",
    "- **Built and trained a simple neural network** to recognize intent from the averaged neural activity.\n",
    "- **Implemented a stimulation function** to boost underactive neurons and synchronize activity between the two regions.\n",
    "- **Visualized and exported the data** so that the results can be further analyzed.\n",
    "\n",
    "This simplified model provides a foundation for exploring how a bidirectional brain-computer interface could work in enhancing inter-regional connectivity, which is especially relevant in conditions like autism spectrum disorder. Future work could expand on these ideas with more detailed simulations, richer datasets, and advanced machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
